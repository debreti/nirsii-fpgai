{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7n_Kjgys-7K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24781914-42d1-40b4-d71c-1c2cb9850476"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/894.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m894.6/894.6 kB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m802.0/802.0 kB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m113.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install \"setuptools<70.0\" -q\n",
        "!pip install brevitas -q\n",
        "\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, torchvision, numpy\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"Torchvision:\", torchvision.__version__)\n",
        "print(\"Numpy:\", numpy.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sxhn2a5ttJ7h",
        "outputId": "1e830383-744c-4f37-ad8d-67c7ce56404b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch: 2.6.0+cu124\n",
            "Torchvision: 0.21.0+cu124\n",
            "Numpy: 1.26.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from brevitas.nn import QuantLinear, QuantReLU\n",
        "from brevitas.quant import Int8WeightPerTensorFixedPoint as WeightQuant\n",
        "from brevitas.quant import Int8ActPerTensorFixedPoint as ActQuant\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 5\n",
        "LEARNING_RATE = 1e-3\n",
        "SAVE_DIR = 'quantized_params'\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "class QuantMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = QuantLinear(28 * 28, 256, bias=True, weight_quant=WeightQuant)\n",
        "        self.relu1 = QuantReLU(act_quant=ActQuant)\n",
        "        self.fc2 = QuantLinear(256, 128, bias=True, weight_quant=WeightQuant)\n",
        "        self.relu2 = QuantReLU(act_quant=ActQuant)\n",
        "        self.fc3 = QuantLinear(128, 10, bias=True, weight_quant=WeightQuant)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = self.relu1(self.fc1(x))\n",
        "        x = self.relu2(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model = QuantMLP()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "model.train()\n",
        "for epoch in range(EPOCHS):\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch [{epoch+1}/{EPOCHS}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "if not os.path.exists(SAVE_DIR):\n",
        "    os.makedirs(SAVE_DIR)\n",
        "\n",
        "layer_idx = 1\n",
        "for name, module in model.named_modules():\n",
        "    if isinstance(module, QuantLinear):\n",
        "        w = module.quant_weight().int().cpu().numpy().astype(np.int8)\n",
        "\n",
        "        b = module.bias.detach().cpu().numpy()\n",
        "        b_scaled = (b * 256).astype(np.int32)\n",
        "\n",
        "        np.save(os.path.join(SAVE_DIR, f'layer{layer_idx}_weight.npy'), w)\n",
        "        np.save(os.path.join(SAVE_DIR, f'layer{layer_idx}_bias.npy'), b_scaled)\n",
        "\n",
        "        print(f\"Layer {layer_idx}: bias min/max = {b_scaled.min()} / {b_scaled.max()}\")\n",
        "        layer_idx += 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7D7yvVXtBM4",
        "outputId": "49d63834-404e-460e-c3c8-6373d8a22227"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:02<00:00, 4.56MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 135kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:06<00:00, 244kB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 7.74MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_tensor.py:1624: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at /pytorch/c10/core/TensorImpl.h:1935.)\n",
            "  return super().rename(names)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Loss: 0.1786\n",
            "Epoch [2/5], Loss: 0.1032\n",
            "Epoch [3/5], Loss: 0.2099\n",
            "Epoch [4/5], Loss: 0.0182\n",
            "Epoch [5/5], Loss: 0.0011\n",
            "Test Accuracy: 97.64%\n",
            "Layer 1: bias min/max = -33 / 42\n",
            "Layer 2: bias min/max = -31 / 43\n",
            "Layer 3: bias min/max = -25 / 28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w1 = np.load('quantized_params/layer1_weight.npy')\n",
        "b1 = np.load('quantized_params/layer1_bias.npy')\n",
        "\n",
        "print(\"Shape of layer1 weights:\", w1.shape)\n",
        "print(\"First few weights:\\n\", w1[:5])\n",
        "\n",
        "print(\"\\nShape of layer1 bias:\", b1.shape)\n",
        "print(\"First few biases:\\n\", b1[:5])\n",
        "\n",
        "print(\"\\nDtype of weights:\", w1.dtype)\n",
        "print(\"Dtype of biases:\", b1.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6_PpWQ3v2MB",
        "outputId": "76cf9f5a-5d1d-4783-d25d-fc5350bcab54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of layer1 weights: (256, 784)\n",
            "First few weights:\n",
            " [[ 2  3 -1 ... -3  3  1]\n",
            " [-2  0  5 ...  3 -1 -3]\n",
            " [-2  3  0 ...  0 -3 -4]\n",
            " [-2 -1 -3 ...  0 -3 -2]\n",
            " [-2  3  2 ... -2 -2 -4]]\n",
            "\n",
            "Shape of layer1 bias: (256,)\n",
            "First few biases:\n",
            " [ 17 -12  17  -1  26]\n",
            "\n",
            "Dtype of weights: int8\n",
            "Dtype of biases: int32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b1[:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Kln8Za1xKvV",
        "outputId": "1d626d8e-a303-4e50-963c-cd74150bdf41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 17, -12,  17,  -1,  26,  18,   1,   7, -16,   1,  -1,   7, -12,\n",
              "        16,  10,   6,  12,  33, -11,  -4,  -5,   5,   8,  14,  17, -15,\n",
              "         5, -20,  -8,  -3, -10,  19,   0,   9,  12, -11,  23,   7,  11,\n",
              "        -2,  10,   5,   4,   0,  24, -19,   1,   0,   2, -10,  21,  16,\n",
              "         3,  18,  -8,  -6,  15,   1,  17,   7,  -7,  17,   2,  22,  23,\n",
              "        28,   5,  -6,  18,   7,  11,  19,  -3, -14,  -2,  27,  16,   3,\n",
              "        17, -18,  14,   1,  15, -10,  -9,  -2,  10, -12,   3,   0,  11,\n",
              "       -11,  -6, -13,  14,  11, -12,   9, -33,  17,  -7, -10,   0,  13,\n",
              "         2,   8,   6,  19,   1, -14,   0,   6,   9,  12,  21,  18,  22,\n",
              "         3,  -8,   4, -13, -10,   2,  24, -23,  27,  -4,  -8,  -7,  -9,\n",
              "         0,  12,   1,   4,  26,   0,  -2,   1,   8,   3,   8,  20, -14,\n",
              "        -1,   1,  18,   3,   5,  -1,  -6,   2,  17,   7,  17,  28,  11,\n",
              "        -4,   6,   2,  -8,  22,  22,   8,  -8,   2,  29,   1,  10,   6,\n",
              "         4,   7,   0,  14,  20, -12,  18,  10,  -5,  -9,  -3,  13,  -4,\n",
              "        -1,   8,  26,  21,  20,  28,  -3,   7,  20,  -9,  11,  23,  10,\n",
              "        -7, -12,   6,  14, -12,   5, -14,  -4,  16,  10,  -2,  -7,  25,\n",
              "         5,  14,  -7,  17,  12,  11,  19,   6, -17,  26,   4,  11,   0,\n",
              "        -9,  -1,  10,   8,   2,  -3,   3, -10,  11,  20,   9,  -4,  -4,\n",
              "        -6,   9,  13,   3,   1,   6,  12,   5,  12,  -4,  23,   3,  -1,\n",
              "        36,  -1,  -5, -13, -28,   0,  42,   3,  18], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SAVE_INFER_DIR = 'inference_samples'\n",
        "os.makedirs(SAVE_INFER_DIR, exist_ok=True)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "samples_to_save = 10\n",
        "saved = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        for i in range(images.size(0)):\n",
        "            image = images[i:i+1].to(device)\n",
        "            label = labels[i].item()\n",
        "\n",
        "            x_int = (image.view(-1) * 255).to(torch.int8).cpu().numpy()\n",
        "\n",
        "            output = model(image)\n",
        "            output_int = output.squeeze().to(torch.int32).cpu().numpy()\n",
        "\n",
        "            pred = int(torch.argmax(output).item())\n",
        "\n",
        "            np.savez(os.path.join(SAVE_INFER_DIR, f'sample_{saved}.npz'),\n",
        "                     input=x_int,\n",
        "                     output=output_int,\n",
        "                     true_label=label,\n",
        "                     pred_label=pred)\n",
        "\n",
        "            saved += 1\n",
        "            if saved >= samples_to_save:\n",
        "                break\n",
        "        if saved >= samples_to_save:\n",
        "            break\n",
        "\n",
        "print(f\"Сохранено {saved} инференс-примеров в папке {SAVE_INFER_DIR}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBmWW3TpyNVr",
        "outputId": "95b241cf-3240-4106-c8e7-974a66f6c15c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Сохранено 10 инференс-примеров в папке inference_samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.load('inference_samples/sample_4.npz')\n",
        "\n",
        "print(\"Входные данные:\")\n",
        "print(data['input'][:20])  # int8, shape (784,)\n",
        "\n",
        "print(\"\\nВыход сети:\")\n",
        "print(data['output'])  # int32, shape (10,)\n",
        "\n",
        "print(\"\\nИстинный класс:\", data['true_label'])\n",
        "print(\"Предсказанный класс:\", data['pred_label'])\n",
        "\n",
        "print(\"\\nТипы данных:\")\n",
        "print(\"input dtype:\", data['input'].dtype)\n",
        "print(\"output dtype:\", data['output'].dtype)"
      ],
      "metadata": {
        "id": "jbXyLpakyeQW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62361516-ba3b-41a3-e872-9b0c16135339"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Входные данные:\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "\n",
            "Выход сети:\n",
            "[ -2  -9  -3 -10  12 -10  -4   2  -8   4]\n",
            "\n",
            "Истинный класс: 4\n",
            "Предсказанный класс: 4\n",
            "\n",
            "Типы данных:\n",
            "input dtype: int8\n",
            "output dtype: int32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "archive_name = 'quantized_mlp_outputs'\n",
        "\n",
        "shutil.make_archive(archive_name, 'zip', root_dir='.', base_dir='quantized_params')\n",
        "shutil.make_archive(archive_name + '_inference', 'zip', root_dir='.', base_dir='inference_samples')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "HCsQOChezAOx",
        "outputId": "1c20c19f-433d-41f2-a902-caf737b3a45f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/quantized_mlp_outputs_inference.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_safe_scale(module):\n",
        "    if hasattr(module, 'scale') and callable(module.scale):\n",
        "        scale = module.scale()\n",
        "        if scale is not None:\n",
        "            return scale.detach().cpu().numpy()\n",
        "    return 1.0\n",
        "\n",
        "def get_layer_scales(model):\n",
        "    requant_scales = {}\n",
        "    layers = list(model.named_modules())\n",
        "    for i, (name, module) in enumerate(layers):\n",
        "        if isinstance(module, QuantLinear):\n",
        "            scale_weight = get_safe_scale(module.weight_quant)\n",
        "\n",
        "            scale_in = 1.0\n",
        "            if i > 0:\n",
        "                _, prev_module = layers[i - 1]\n",
        "                scale_in = get_safe_scale(prev_module)\n",
        "\n",
        "            scale_out = 1.0\n",
        "            if i + 1 < len(layers):\n",
        "                _, next_module = layers[i + 1]\n",
        "                scale_out = get_safe_scale(next_module)\n",
        "\n",
        "            requant_scale = (scale_in * scale_weight) / scale_out\n",
        "            requant_scales[name] = requant_scale\n",
        "            print(f\"{name}: requant_scale = {requant_scale}\")\n",
        "\n",
        "    return requant_scales\n",
        "\n",
        "requant_scales = get_layer_scales(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOlS0CD_tanw",
        "outputId": "e145c99f-1aa2-4599-ee70-69e04a9a264e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fc1: requant_scale = 0.0078125\n",
            "fc2: requant_scale = 0.0078125\n",
            "fc3: requant_scale = 0.00390625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zzo5pM471nuQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}